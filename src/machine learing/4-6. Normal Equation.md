正規化方程。在之前我們都是使用迭代的算法來求的最佳的擬合點。不過有種方式能直接的計算出擬合點，不需要使用迭代算法。

下圖為梯度下降法
![[Pasted image 20230923205011.png]]



---
這邊有個問題是說，對每一個參數求導後，就能得到最佳解。並且通常維度並不會是一個實數，不太懂他所說的意思。
![[Pasted image 20230923210058.png]]

---


假設我們有一個資料集，他有4筆的資料。
![[Pasted image 20230923211458.png]]

我們可以把它寫為矩陣
$$X=
\begin{bmatrix}
1 & 2104 & 5 & 1 & 45 \\
1 & 1416 & 3 & 2 & 40 \\
1 & 1534 & 3 & 2 & 30 \\
1 & 852  & 2 & 1 & 36
\end{bmatrix}$$
$$y=\\
\begin{bmatrix}
460 \\
232\\
315\\
178
\end{bmatrix}
$$

$\theta$公式：
$$\theta = (X^TX)^{-1}X^Ty$$
將其帶入後變成：
![[Pasted image 20230923212119.png]]
之後就可以得到$\theta$的解答

---
在上面的運算後，我們會得出4個元素的$\theta$矩陣，但是在$h(x)$裡面不是只有一個$\theta$嗎？

---
### 比較

在正規化方程裡面有很多的限制，例如該矩陣不可逆，預測的算法並不是線性的，資料數大於10,000等等。

| 梯度下降                   | 正规方程                                                                                                   |
| -------------------------- | ---------------------------------------------------------------------------------------------------------- |
| 需要选择学习率             | 不需要                                                                                                     |
| 需要多次迭代               | 一次运算得出                                                                                               |
| 当特征数量大时也能较好适用 | 需要计算 如果特征数量n较大则运算代价大，因为矩阵逆的计算时间复杂度为，通常来说当小于10000 时还是可以接受的 |
| 适用于各种类型的模型       | 只适用于线性模型，不适合逻辑回归模型等其他模型                                                             |           



> 随着我们要讲的学习算法越来越复杂，例如，当我们讲到分类算法，像逻辑回归算法，我们会看到，实际上对于那些算法，并不能使用标准方程法。对于那些更复杂的学习算法，我们将不得不仍然使用梯度下降法。因此，梯度下降法是一个非常有用的算法，可以用在有大量特征变量的线性回归问题。或者我们以后在课程中，会讲到的一些其他的算法，因为标准方程法不适合或者不能用在它们上。但对于这个特定的线性回归模型，标准方程法是一个比梯度下降法更快的替代算法。所以，根据具体的问题，以及你的特征变量的数量，这两种算法都是值得学习的。


