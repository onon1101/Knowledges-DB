目的將所有訓練集的建模誤差控制在最小

![[Pasted image 20230825222258.png]]

在 h 函數裡面$\theta_0 \ \theta_1$也稱為**参数**（**parameters**）
在上圖 每一點與我們預測的h函數稱為**建模误差**（**modeling error**）

### 如何設計cost function
1. 取每一筆資料與h 函數的誤差$$h_{\theta}(x^i) - y^i$$
2. 將誤差放大，而最簡單的方式就是平方$$(h_{\theta}(x^i) - y^i)^2$$
3. 全部的誤差累積起來，在算出平均$$\frac{1}{2m}\sum_{i=1}^{m}(h_{\theta}(x^i) - y^i)^2$$
4. 完成cost function $$J(\theta_0,\theta_1)=\frac{1}{2m}\sum_{i=1}^{m}(h_{\theta}(x^i) - y^i)^2$$
別忘了$h_{\theta}(x)=\theta_{0}+\theta_{1}x$

所以才會需要兩個參數
![[Pasted image 20230825223250.png]]

從圖中就可以看出最低的地方就是最完美的解答，因為他所代表的誤差是最低的

---
### 簡單點的理解
![[Pasted image 20230825223441.png]]
假設h 函數只有$\theta_1$一個參數, 當我們梅舉所有的可能時，會發現存在一組解他是所有可能裡面建模誤差最低的